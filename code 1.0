# -*- coding: utf-8 -*-
"""
Created on Wed Feb  7 18:19:54 2024

@author: natha sarah la bg et zoozozozoozozozozozozoozoozozozozozoz
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pandas as pd
import schedule
import time

# Définir la plage de saisons que vous voulez
saisons = ['2023', '2024']

# Définir les dates de début et de fin pour chaque saison
dates_debut = [datetime(int(saison), 8, 11) for saison in saisons]
dates_fin = [datetime(int(saison)+1, 5, 19) for saison in saisons]  # Update the end date to May 19, 2024

# Construire les URLs pour chaque saison et date
base_url = 'https://www.espn.co.uk/football/scoreboard/_/date/'
user_agent = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0'}

def scrape_and_save_scores():
    # Créer une liste pour stocker les résultats des matchs
    match_data = []

    for saison, date_debut, date_fin in zip(saisons, dates_debut, dates_fin):
        current_date = date_debut

        while current_date <= date_fin and current_date.date() <= datetime(2024, 5, 19).date():  # Update the comparison date
            date_formattee = current_date.strftime('%Y%m%d')
            url = f'{base_url}{date_formattee}/league/eng.1'

            # Faire une requête HTTP à l'URL
            response = requests.get(url, headers=user_agent)

            if response.status_code == 200:
                # Utiliser BeautifulSoup pour extraire le code HTML
                soup = BeautifulSoup(response.content, 'html.parser')

                # Extraire la date
                try:
                    date_element = soup.find('h3', class_='Card__Header__Title')
                    match_date = date_element.text.strip()
                except Exception as date_error:
                    print(f'Erreur lors de l\'extraction de la date : {date_error}')
                    match_date = 'N/A'

                # Extraire les informations sur les équipes et les scores
                match_containers = soup.select('.ScoreboardScoreCell')  # Assuming each match is in a div with class 'ScoreboardScoreCell'

                # Iterate over each match container
                for match_container in match_containers:
                    home_team_element = match_container.select_one('.ScoreCell__TeamName--shortDisplayName')
                    home_team_name = home_team_element.text.strip() if home_team_element else 'N/A'

                    home_team_score_element = match_container.select_one('.ScoreCell_Score--scoreboard')
                    home_team_score = home_team_score_element.text.strip() if home_team_score_element else 'N/A'

                    away_team_elements = match_container.select('.ScoreCell__TeamName--shortDisplayName')
                    away_team_scores = match_container.select('.ScoreCell_Score--scoreboard')

                    away_team_name = away_team_elements[-1].text.strip() if away_team_elements else 'N/A'
                    away_team_score = away_team_scores[-1].text.strip() if away_team_scores else 'N/A'

                    # Ajouter les informations du match à la liste
                    match_data.append({
                        'Date': match_date,
                        'Home Team': home_team_name,
                        'Home Team Score': home_team_score,
                        'Away Team': away_team_name,
                        'Away Team Score': away_team_score
                    })

            else:
                print(f'Erreur lors de la requête HTTP pour l\'URL {url}, Status Code: {response.status_code}')

            # Passer à la date suivante
            current_date += timedelta(days=1)

    # Créer un DataFrame à partir des données des matchs
    match_data_df = pd.DataFrame(match_data)

    # Sauvegarder le DataFrame dans un fichier Excel
    excel_path = r'C:\Users\natha\OneDrive\Bureau\M1\resultats_matchs.xlsx'
    match_data_df.to_excel(excel_path, index=False)

    print(f'Les résultats des matchs ont été enregistrés dans : {excel_path}')

# Planifier la tâche de scraping chaque jour à une heure spécifique
schedule.every().day.at("18:21").do(scrape_and_save_scores)

# Boucle pour exécuter la planification en continu
while True:
    schedule.run_pending()
    time.sleep(1)  # Pause d'une seconde entre les vérifications
